{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "frameworkTDIA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIfn-rmnwVmU"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import reduce\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import gzip\n",
        "import os\n",
        "import pickle\n",
        "from urllib import request"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W27iQrv0dm4W"
      },
      "source": [
        "filename = [\n",
        "    [\"training_images\", \"train-images-idx3-ubyte.gz\"],\n",
        "    [\"test_images\", \"t10k-images-idx3-ubyte.gz\"],\n",
        "    [\"training_labels\", \"train-labels-idx1-ubyte.gz\"],\n",
        "    [\"test_labels\", \"t10k-labels-idx1-ubyte.gz\"]\n",
        "]\n",
        "\n",
        "\n",
        "def download_mnist():\n",
        "  base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "  for name in filename:\n",
        "    print(\"Downloading \" + name[1] + \"...\")\n",
        "    request.urlretrieve(base_url + name[1], name[1])\n",
        "  print(\"Download complete.\")\n",
        "\n",
        "\"\"\"\n",
        "def mnist_init():\n",
        "  mnist = {}\n",
        "  for name in filename[:2]:\n",
        "    with gzip.open(name[1], 'rb') as f:\n",
        "        mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28 * 28)\n",
        "  for name in filename[-2:]:\n",
        "    with gzip.open(name[1], 'rb') as f:\n",
        "        mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "  return mnist\n",
        "\"\"\"\n",
        "\n",
        "def mnist_init():\n",
        "    if not os.path.isfile(\"mnist.pkl\"):\n",
        "        download_mnist()\n",
        "        save_mnist()\n",
        "    else:\n",
        "        print(\"Dataset already downloaded, delete mnist.pkl if you want to re-download.\")\n",
        "\n",
        "def save_mnist():\n",
        "    mnist = {}\n",
        "    for name in filename[:2]:\n",
        "        with gzip.open(name[1], 'rb') as f:\n",
        "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28 * 28)\n",
        "    for name in filename[-2:]:\n",
        "        with gzip.open(name[1], 'rb') as f:\n",
        "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "    for _, gz_file in filename:\n",
        "        os.remove(gz_file)\n",
        "\n",
        "    with open(\"mnist.pkl\", 'wb') as f:\n",
        "        pickle.dump(mnist, f)\n",
        "    print(\"Save complete.\")\n",
        "\n",
        "def mnist_load():\n",
        "    with open(\"mnist.pkl\", 'rb') as f:\n",
        "        mnist = pickle.load(f)\n",
        "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNjoL9yVwL7T"
      },
      "source": [
        "\n",
        "class Sigmoid:\n",
        "  def apply(self, x):\n",
        "    return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
        "\n",
        "  def df(self, x):\n",
        "    y = self.apply(x)\n",
        "    return y * (1 - y)\n",
        "\n",
        "\n",
        "class Relu:\n",
        "  def apply(self, x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "  def df(self, x):\n",
        "    return np.where(x <= 0, 0, 1)\n",
        "\n",
        "\n",
        "class SoftMax:\n",
        "  def apply(self, x):\n",
        "    y = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return y / np.sum(y, axis=1, keepdims=True)\n",
        "\n",
        "sigmoid = Sigmoid()\n",
        "relu = Relu()\n",
        "softmax = SoftMax()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQjWV6ElqshZ"
      },
      "source": [
        "class FullyConnected:\n",
        "\n",
        "  def __init__(self, size, activation):\n",
        "    self.size = size\n",
        "    self.activation = activation\n",
        "    self.is_softmax = isinstance(self.activation, SoftMax)\n",
        "    self.cache_aprev,self.cache_z,self.cache_a=None,None,None\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "\n",
        "  def init(self, in_dim):\n",
        "    # He initialization\n",
        "    self.w = np.random.randn(self.size, in_dim) * np.sqrt(2 / in_dim)\n",
        "    self.b = np.zeros((1, self.size))\n",
        "    print(\"in\",in_dim)\n",
        "    print(\"in\",self.size)\n",
        "\n",
        "  def forward(self, a_prev, training):\n",
        "    z = np.dot(a_prev, self.w.T) + self.b\n",
        "    a = self.activation.apply(z)\n",
        "\n",
        "    if training:\n",
        "      self.cache_aprev,self.cache_z,self.cache_a= a_prev, z,a\n",
        "\n",
        "    return a\n",
        "\n",
        "  def backward(self, da):\n",
        "    a_prev, z,a = self.cache_aprev,self.cache_z,self.cache_a\n",
        "    batch_size = a_prev.shape[0]\n",
        "\n",
        "    if self.is_softmax:\n",
        "      y = da * (-a)\n",
        "      dz = a - y\n",
        "    else:\n",
        "      dz = da * self.activation.df(z)\n",
        "\n",
        "    dw = 1 / batch_size * np.dot(dz.T, a_prev)\n",
        "    db = 1 / batch_size * dz.sum(axis=0, keepdims=True)\n",
        "    da_prev = np.dot(dz, self.w)\n",
        "\n",
        "    return da_prev, dw, db\n",
        "\n",
        "  def update_params(self, dw, db):\n",
        "    self.w -= dw\n",
        "    self.b -= db\n",
        "\n",
        "  def get_params(self):\n",
        "    return self.w, self.b\n",
        "\n",
        "  def get_output_dim(self):\n",
        "    return self.size\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MOWqYF9fvCt"
      },
      "source": [
        "epsilon = 1e-20\n",
        "\n",
        "class SoftmaxCrossEntropy:\n",
        "  def apply(self, a_last, y):\n",
        "    batch_size = y.shape[0]\n",
        "    cost = -1 / batch_size * (y * np.log(np.clip(a_last, epsilon, 1.0))).sum()\n",
        "    return cost\n",
        "  \"\"\"\n",
        "  def grad(self, a_last, y):\n",
        "    return a_last-y\n",
        "  \"\"\"\n",
        "  def grad(self, a_last, y):\n",
        "    return - np.divide(y, np.clip(a_last, epsilon, 1.0))  \n",
        "\n",
        "\n",
        "class SigmoidCrossEntropy:\n",
        "    def apply(self, a_last, y):\n",
        "        batch_size = y.shape[0]\n",
        "        # It would be better to have the logits and use this instead\n",
        "        # max(logits, 0) - logits * y + log(1 + exp(-abs(logits)))\n",
        "        a_last = np.clip(a_last, epsilon, 1.0 - epsilon)\n",
        "        cost = -1 / batch_size * (y * np.log(a_last) + (1 - y) * np.log(1 - a_last)).sum()\n",
        "        return cost\n",
        "\n",
        "    def grad(self, a_last, y):\n",
        "        a_last = np.clip(a_last, epsilon, 1.0 - epsilon)\n",
        "        return - (np.divide(y, a_last) - np.divide(1 - y, 1 - a_last))\n",
        "\n",
        "softmax_cross_entropy = SoftmaxCrossEntropy()\n",
        "sigmoid_cross_entropy = SigmoidCrossEntropy()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp0gBR1Mh36D"
      },
      "source": [
        "class GradientDescent:\n",
        "  def __init__(self, trainable_layers):\n",
        "    self.trainable_layers = trainable_layers\n",
        "\n",
        "  def initialize(self):\n",
        "    pass\n",
        "\n",
        "  def update(self, learning_rate, w_grads, b_grads, step):\n",
        "    for layer in self.trainable_layers:\n",
        "      layer.update_params(dw=learning_rate * w_grads[layer],db=learning_rate * b_grads[layer])\n",
        "\n",
        "gradient_descent = GradientDescent"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf-tS1SbhdAk"
      },
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, input_dim, layers, cost_function, optimizer=gradient_descent):\n",
        "    self.layers = layers\n",
        "    self.w_grads = {}\n",
        "    self.b_grads = {}\n",
        "    self.cost_function = cost_function\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "    self.layers[0].init(input_dim)\n",
        "\n",
        "    for prev_layer, curr_layer in zip(self.layers, self.layers[1:]):\n",
        "      curr_layer.init(prev_layer.get_output_dim())\n",
        "\n",
        "    self.trainable_layers = set(layer for layer in self.layers if layer.get_params() is not None)\n",
        "    self.optimizer = optimizer(self.trainable_layers)\n",
        "    self.optimizer.initialize()\n",
        "\n",
        "  def forward_prop(self, x, training=True):\n",
        "    a = x\n",
        "    for layer in self.layers:\n",
        "      a = layer.forward(a, training)\n",
        "    return a\n",
        "  \n",
        "  def backward_prop(self, a_last, y):\n",
        "    da=self.cost_function.grad(a_last, y)\n",
        "    batch_size = da.shape[0]\n",
        "    \n",
        "    for layer in reversed(self.layers):\n",
        "      da_prev, dw, db = layer.backward(da)\n",
        "      \n",
        "      if layer in self.trainable_layers:\n",
        "        self.w_grads[layer] = dw\n",
        "        self.b_grads[layer] = db\n",
        "\n",
        "      da = da_prev\n",
        "  \n",
        "  def predict(self, x):\n",
        "    a_last = self.forward_prop(x, training=False)\n",
        "    return a_last\n",
        "  \n",
        "  def update_param(self, learning_rate, step):\n",
        "    self.optimizer.update(learning_rate, self.w_grads, self.b_grads, step)\n",
        "  \n",
        "  def compute_cost(self, a_last, y):\n",
        "    cost = self.cost_function.apply(a_last, y)\n",
        "    return cost\n",
        "\n",
        "  \n",
        "  def train_step(self, x_train, y_train, learning_rate, step):\n",
        "    a_last = self.forward_prop(x_train, training=True)\n",
        "    self.backward_prop(a_last, y_train)\n",
        "    self.update_param(learning_rate, step)\n",
        "    \n",
        "  def train(self, x_train, y_train, learning_rate, num_epochs, test_data):\n",
        "    x_test, y_test = test_data\n",
        "    costs=[]\n",
        "    costs_test=[]\n",
        "    accuracyTrain=[]\n",
        "    accuracyTest=[]\n",
        "    step = 0\n",
        "    for e in range(num_epochs):\n",
        "      print(\"Epoch \" + str(e + 1))\n",
        "      epoch_cost = 0\n",
        "\n",
        "      step += 1\n",
        "      self.train_step(x_train, y_train, learning_rate, step)\n",
        "      \n",
        "      costTest,accuracy_test=self.validate_model(x_test, y_test)\n",
        "      costs_test.append(costTest)\n",
        "      accuracyTest.append(accuracy_test)\n",
        "\n",
        "      cost,accuracy_train=self.validate_model(x_train, y_train)\n",
        "      costs.append(cost)\n",
        "      accuracyTrain.append(accuracy_train)\n",
        "      \n",
        "      print(f\"\\nCost after epoch {e+1}: test= {costTest} , train = {cost}\")\n",
        "      \n",
        "      print(f\"Accuracy on test set: {accuracy_test}\")\n",
        "      print(\"Finished training\")\n",
        "    self.visualizeCost(costs,costs_test)\n",
        "    self.visualizeAccuracy(accuracyTrain,accuracyTest)\n",
        "\n",
        "  def validate_model(self,x, y):\n",
        "    cost=0\n",
        "    accuracy=0\n",
        "    size=x.shape[0]\n",
        "    prediction=self.predict(x)#argmax no one hot\n",
        "    cost+=self.cost_function.apply(prediction, y)\n",
        "    accuracy+=np.sum(np.argmax(prediction, axis=1) == np.argmax(y, axis=1))\n",
        "    accuracy=accuracy/size\n",
        "    return cost,accuracy\n",
        "  \n",
        "  def visualizeCost(self,costs,costs_test):\n",
        "    plt.plot(costs)\n",
        "    plt.plot(costs_test)\n",
        "    plt.ylabel('cost')\n",
        "    plt.legend([\"train\", \"test\"])\n",
        "    plt.show()\n",
        "  \n",
        "  def visualizeAccuracy(self,accuracyTrain,accuracyTest):\n",
        "    plt.plot(accuracyTrain)\n",
        "    plt.plot(accuracyTest)\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend([\"train\", \"test\"])\n",
        "    plt.show()\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrJxK9UZEV96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e26e255-0060-4865-ccf8-42100a6ea027"
      },
      "source": [
        "def one_hot(x, num_classes=10):\n",
        "  out = np.zeros((x.shape[0], num_classes))\n",
        "  out[np.arange(x.shape[0]), x[:, 0]] = 1\n",
        "  return out\n",
        "\n",
        "def preprocess(x_train, y_train, x_test, y_test):\n",
        "  x_train = x_train.reshape(x_train.shape[0], 28* 28).astype(np.float32)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 28* 28).astype(np.float32)\n",
        "  y_train = one_hot(y_train.reshape(y_train.shape[0], 1))\n",
        "  y_test = one_hot(y_test.reshape(y_test.shape[0], 1))\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  return x_train, y_train, x_test, y_test\n",
        "\n",
        "mnist_init()\n",
        "x_train, y_train, x_test, y_test= preprocess(*mnist_load())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already downloaded, delete mnist.pkl if you want to re-download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWJgX6_GeOH4"
      },
      "source": [
        "\n",
        "#conv c_filter_size,c_channels,c_filters,c_stride,c_pad,activation=Relu()\n",
        "cnn = NeuralNetwork(\n",
        "    input_dim=(x_train.shape[1]),\n",
        "    layers=[\n",
        "            FullyConnected(128, relu),\n",
        "            FullyConnected(64, relu),\n",
        "            FullyConnected(10, softmax)      \n",
        "        ],\n",
        "        cost_function=softmax_cross_entropy,\n",
        "        optimizer=gradient_descent\n",
        "        \n",
        "    )\n",
        "\n",
        "cnn.train(x_train, y_train,\n",
        "          learning_rate=0.01,\n",
        "          num_epochs=100,\n",
        "          test_data=(x_test, y_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h1rYvOhLudZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e4a8a0a-4f86-4a44-abb4-a204dfe5fb7c"
      },
      "source": [
        "pred_test=np.argmax(cnn.predict(x_test),axis=1)\n",
        "print(classification_report(np.argmax(y_test, axis=1), pred_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.90      0.77       980\n",
            "           1       0.66      0.98      0.79      1135\n",
            "           2       0.76      0.69      0.72      1032\n",
            "           3       0.87      0.48      0.62      1010\n",
            "           4       0.50      0.57      0.53       982\n",
            "           5       0.65      0.33      0.44       892\n",
            "           6       0.79      0.80      0.80       958\n",
            "           7       0.74      0.34      0.46      1028\n",
            "           8       0.50      0.71      0.59       974\n",
            "           9       0.54      0.62      0.58      1009\n",
            "\n",
            "    accuracy                           0.65     10000\n",
            "   macro avg       0.67      0.64      0.63     10000\n",
            "weighted avg       0.67      0.65      0.63     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aers-A-vBMhG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "f0a542af-7f43-49c4-bd22-c4d214d30403"
      },
      "source": [
        "\"\"\"\n",
        "def preprocess1(x_train, y_train, x_test, y_test):\n",
        "  x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=0.15)\n",
        "  x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype(np.float32)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype(np.float32)\n",
        "  x_dev = x_dev.reshape(x_dev.shape[0], 28, 28, 1).astype(np.float32)\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  x_dev /= 255\n",
        "  return x_train, y_train, x_test, y_test,x_dev,y_dev\n",
        "\n",
        "mnist_init()\n",
        "x_train, y_train, x_test, y_test,x_dev,y_dev = preprocess1(*mnist_load())\n",
        "pred_train=np.argmax(cnn.predict(x_train),axis=1)\n",
        "print(classification_report(y_train, pred_train))\n",
        "\"\"\""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef preprocess1(x_train, y_train, x_test, y_test):\\n  x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=0.15)\\n  x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype(np.float32)\\n  x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype(np.float32)\\n  x_dev = x_dev.reshape(x_dev.shape[0], 28, 28, 1).astype(np.float32)\\n  x_train /= 255\\n  x_test /= 255\\n  x_dev /= 255\\n  return x_train, y_train, x_test, y_test,x_dev,y_dev\\n\\nmnist_init()\\nx_train, y_train, x_test, y_test,x_dev,y_dev = preprocess1(*mnist_load())\\npred_train=np.argmax(cnn.predict(x_train),axis=1)\\nprint(classification_report(y_train, pred_train))\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}